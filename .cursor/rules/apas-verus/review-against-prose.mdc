# Review Chapter Against Prose

When the user says "review ChapNN" or "review against prose":

## Phase 1: Inventory (tool-generated)

Run veracity to generate the function inventory and spec strengths:

```bash
~/projects/veracity/target/release/veracity-review-module-fn-impls -d src/ChapNN
```

Then classify spec strengths per the `classify-spec-strengths` rule. This gives us:
- Every function in the chapter
- Whether it's in a trait, impl, or module-level
- Whether it's inside `verus!`
- Spec strength (strong / partial / weak / none)
- Proof holes

This is the mechanical baseline. Don't duplicate what the tool already produces.

## Phase 2: Prose Inventory (manual)

Read `prompts/ChapNN.txt`. Extract every named item into categories:

- **Definitions**: Named ADTs, type classes, abstract interfaces
- **Algorithms**: Pseudocode with a name (e.g., "Algorithm 21.1", "insertion sort")
- **Cost specs**: Every stated Work and Span bound
- **Theorems/Properties**: Correctness claims, invariants, bounds
- **Exercises/Problems**: Numbered items that the code may implement

## Phase 3: Algorithmic Analysis (the review)

For each executable function (`fn`, not `spec fn` or `proof fn`):

### 3a. Cost annotations in source

Write two doc comment lines directly before the function:

```rust
/// - APAS: Work Θ(...), Span Θ(...)
/// - Claude-Opus-4.6: Work Θ(...), Span Θ(...) — [reason if different]
```

Rules for the two lines:

**APAS line**: What the textbook says the cost *should* be for this algorithm.
If the prose doesn't state a cost for this specific function, write:
```rust
/// - APAS: (no cost stated)
```
If the function has no prose counterpart at all, write:
```rust
/// - APAS: N/A — Verus-specific scaffolding.
```

**Claude-Opus-4.6 line**: What the code *actually* achieves, based on reading the
implementation. Three outcomes:

1. **Agree**: `/// - Claude-Opus-4.6: Work Θ(...), Span Θ(...) — agrees with APAS.`
2. **Disagree**: `/// - Claude-Opus-4.6: Work Θ(...), Span Θ(...) — [specific reason for difference]`
3. **Cannot determine**: `/// - Claude-Opus-4.6: Cost not analyzable — [reason, e.g., external_body]`

The disagreement reason must be concrete. Not "differs from APAS" but
"closures call fib_seq not fib_par, so only top-level split is parallel"
or "uses linear scan where APAS assumes O(1) hash lookup."

### 3b. Implementation fidelity

For each function that implements a prose algorithm, note whether the code
follows the prose algorithm or deviates. Deviations are not necessarily wrong —
but they must be noted because they can change the cost.

Common deviations:
- Sequential where APAS says parallel (or vice versa)
- Different data structure than APAS assumes (e.g., Vec where APAS says array with O(1) slice)
- Missing recursive parallelism (like fib_par calling fib_seq)
- Granularity cutoffs not in the prose (acceptable, note them)

### 3c. Spec fidelity

For each function with requires/ensures, compare against the prose:
- Does `requires` capture the prose's stated preconditions?
- Does `ensures` capture the prose's stated postconditions?
- Are there prose properties that the spec doesn't express?

This complements the veracity spec-strength classification — spec strength
tells you whether a spec exists and how complete it looks structurally;
spec fidelity tells you whether it matches what the textbook actually claims.

## Phase 4: Parallelism Review

For every `*Mt*` (multi-threaded) module, check whether each operation is
genuinely parallel or just thread-safe.

### 4a. Classify each Mt function

For each exec function in a `*Mt*` module, determine:

1. **Parallel** — spawns threads, uses `join`, or calls `WSSchedulerMtEph`
   spawn/wait. The Span reflects actual parallelism.
2. **Sequential** — uses a sequential loop (for/while/loop) with no spawning.
   Thread-safe (Send + Sync bounds) but Span == Work. The APAS Span annotation
   may be aspirational rather than achieved.
3. **Delegating** — calls the St (single-threaded) variant or another sequential
   helper. Same as sequential.

### 4b. Span audit

For each Mt function annotated with `Span Θ(...)`:
- If the function is parallel, verify the Span matches the parallelism structure
  (e.g., spawn-per-element gives Span Θ(element-cost), fork-join gives Span
  Θ(log n × element-cost)).
- If the function is sequential, the Span equals the Work. Flag any annotation
  where Span < Work — that Span is aspirational, not achieved.
- Note: an aspirational Span is not wrong in the APAS line (it's what the
  textbook *intends*), but the Claude-Opus-4.6 line must state the actual Span.

### 4c. Parallelism gap table

Produce a table:

| Function | APAS Span | Actual | Parallel? | Notes |
|----------|-----------|--------|-----------|-------|

This makes it immediately visible which Mt operations still need parallel
implementations.

## Phase 5: Runtime Test Review


Check that every chapter module has a corresponding runtime test file in `tests/ChapNN/`.

For each source module `src/ChapNN/FooStBar.rs`, expect `tests/ChapNN/TestFooStBar.rs`.

### 4a. Coverage check

- List all exec functions in the module (from Phase 1 inventory).
- List all test functions in the test file.
- Flag exec functions with no test coverage.
- Flag test functions that test deleted or renamed functions.

### 4b. Test quality

For each test, assess:
- Does it exercise the happy path (valid inputs, expected outputs)?
- Does it exercise edge cases (empty inputs, singleton, boundary values)?
- Does it test the spec-relevant properties (the things `ensures` promises)?
- For types with `PartialEq`, does it test equality?

### 4c. Missing tests

Propose new tests for uncovered exec functions. Prioritize:
1. Functions with strong specs — a runtime test validates the spec informally.
2. Functions with proof holes (`external_body`, `assume`) — a runtime test is the
   only evidence the implementation is correct.
3. Functions used as building blocks by other chapters.

## Phase 6: Gap Analysis

Two lists:

**Prose items with no implementation:**
- Algorithm X.Y defined in prose but no corresponding function in code
- Theorem stated but not proved (no lemma)
- Cost bound stated but not annotated

**Code with no prose counterpart:**
- Helper functions, Verus scaffolding, overflow lemmas
- These are expected — just note them so the inventory is complete

## Output

- **Cost annotations**: Written directly in source files as doc comments (Phase 3a)
- **Everything else**: Tool output goes to `analyses/` per the classify-spec-strengths rule
- **Gap analysis and fidelity notes**: Go in the chat response, not in files,
  unless the user asks for a markdown writeup

## Do NOT

- Modify implementation logic, requires/ensures, or function signatures
- Skip the veracity tool and hand-build function inventories
- Write APAS cost lines without reading the prose first
- Write Claude-Opus-4.6 cost lines without reading the implementation
- Leave a function with an APAS cost line but no Claude-Opus-4.6 line (always pair them)
