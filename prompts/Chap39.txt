Chapter 39
Treaps
The parametric data structure presented in the previous Chapter established an interesting
observation: to implement the BST ADT, we only need to provide an implementation of
joinMid .
In this chapter, we implement the 38.1 interface based on a data structure called Treaps
(tree heaps). We show that joinMid , and split based on it, take O(log n) work (and span).
Treaps achieve their efficiency by maintaining BSTs that are probabilistically balanced. Of
the many balanced BST data structures, Treaps are likely the simplest, but, since they are
randomized, they only guarantee approximate balance, though with high probability.
1 Treap Properties
The idea behind Treaps is to associate each key with a randomly selected priority. Then and
in addition to maintaining the BST property on the keys, treaps maintain a “heap ordering”
on these priorities. The heap ordering is the same as you might have seen when studying
binary heaps, leading to the name“Tree Heap” or shortened to “Treap.”
Definition 39.1 (Treap). A Treap is a binary search tree over a set K along with a priority
for each key given by
p : K → Z
that in addition to satisfying the BST property on the keys K, satisfies the heap property
on the priorities p(k), k ∈ K. In particular for every internal node u of the tree that has a
parent node v:
p(k(v)) ≥ p(k(u))
2

where k(v) denotes the key of a node. This simply states that the priority of the parent is
greater than the priorities of its children.
Example 39.1. The following key-priority pairs (k, p(k)),
(a, 3), (b, 9), (c, 2), (e, 6), (f, 5) ,
where the keys are ordered alphabetically, form the following Treap:
since 9 is larger than 3 and 6, and 6 is larger than 2 and 5.
Assigning Priorities. So how do we assign priorities? As we briefly suggested in the
informal discussion above, it turns out that if the priorities are selected randomly then the
tree is guaranteed to be near balanced, i.e. O(lg |S|) height, with high probability. We will
show this shortly. Since we are using a function p(·) to generate the priorities, we assume
it is a random function (i.e., it always returns the same integer for a given key, but which
value it returns is random).
We will assume the priorities are unique—i.e., every distinct key maps to a unique priority.
This is not necessary for the algorithms and bounds given in this chapter, but it simplifies
the description and analysis.
The Treap Type. In our discussion we will use the following recursive type for the defi-
nition of a BST type based on treaps.
type T = TLeaf
| TNode of (T × K × Z × T)
where the TNode type consists of a 4 tuple (L, k, p, R), with L as the left child, k as the key,
p as the priority, and R as the right child. We use TLeaf and TNode to distinguish them

from Leaf and Node in the parametric tree interface.
Exercise 39.1. Prove that if the priorities are unique, then there is exactly one tree structure
that satisfies the Treap properties.
2. HEIGHT ANALYSIS OF TREAPS 281
2 Height Analysis of Treaps
We can analyze the height of Treaps by relating their structure to the recursion tree of
quicksort, which we have already studied.
Algorithm 39.2 (Treap Generating Quicksort). The following variant of quicksort generates
a treap. This algorithm is almost identical to our previous quicksort except that it uses
Node instead of append , and because it is generating a treap consisting of unique keys, the
algorithm retains only one key equaling the pivot.
1 qsTree a =
2 if |a| = 0 then TLeaf
3 else let
4 k = the key k ∈ a for which p(k) is the largest
5 L = 〈 x ∈ a | x < k 〉
6 R = 〈 x ∈ a | x > k 〉
7 (L′, R′) = (qsTree L) || (qsTree R)
8 in
9 TNode (L′, k, p(k), R′)
10 end

The tree generated by qsTree(a) is the Treap for the sequence a. This can be seen by induc-
tion. It is true for the base case. Now assume by induction it is true for the trees returned
by the two recursive calls. The tree returned by the main call is then also a Treap since the
pivot x has the highest priority, and therefore is correctly placed at the root, the subtrees
and in heap order by induction, and because the keys in l are less than the pivot, and the
keys in r are greater than the pivot, the tree has the BST property.
Based on this isomorphism, we can bound the height of a Treap by the recursion depth of
quicksort. Recall that when studying the order statistics problem we proved that if we pick
the priorities at random, the recursion depth is O(lg n) with high probability. Based on this
fact, and by using union bound , we then proved that the depth of the quicksort recursion
(pivot) tree is logarithmic—O(lg n)—with high probability. We therefore conclude that
that the height of a Treap is O(lg n) with high probability.
3 The Treap Data Structure
We are now ready to implement the 38.1 ADT with Treaps. In particular we need an al-
gorithm for the joinMid function. It must maintain the Treap invariants. We hold off on
implementing a size function until the next chapter when we discuss augmenting trees.
Our Treap implementation of the parametric BST ADT is defined as follows.


Data Structure 39.3 (Treaps). An implementation of the 38.1 ADT.
type K
type T = TLeaf | TNode of (T × K × Z × T)
type E = Leaf | Node of (T × K × T)
priority T =
case T of
TLeaf ⇒ −∞
| TNode(L, k, p, R) ⇒ p
join(T1, (k, p), T2) : T × (K × Z) × T → T =
if (p > priority(T1)) ∧ (p > priority(T2)) then
TNode(T1, k, p, T2)
else if (priority(T1) > priority(T2)) then
case T1 of TNode(L1, k1, p1, R1)
⇒ TNode(L1, k1, p1, join(R1, (k, p), T2))
else
case T2 of TNode(L2, k2, p2, R2)
⇒ TNode(join(T1, (k, p), L2), k2, p2, R2)
expose T : T → E =
case T of
TLeaf ⇒ Leaf
| TNode(L, k, , R) ⇒ Node(L, k, R)
joinMid T : E → T =
case T of
Leaf ⇒ TLeaf
| Node(L, k, R) ⇒ join(L, (k, p(k)), R)
Join Algorithm. We now consider the algorithm for join(T1, (k, p), T2). Here p is the pri-
ority of the key k. Due to the requirements of joinMid we are ensured that T1 < k < T2,
and we assume that T1 and T2 each satisfy the treap invariants (i.e., BSTs and heap ordered
priorities). To maintain the treap invariants on the results, we not only need to maintain
the ordering for a BST, but also need to maintain the heap property. In the following dis-
cussion, we refer to the priority of a tree as the priority of its root if it is a node, or −∞ if it
is a leaf. The helper function priority(T ) returns this priority using O(1) work.
To maintain the heap property, the algorithm first checks if it is “lucky” and priority p is al-
ready greater than the priority of T1 and T2. In this case it can just make a node directly and
is done. If not, it then needs to check which of T1 and T2 has a higher priority since the root
of that one needs to become the overall root. In the first case priority(T1) > priority(T2).
In this case since the priority is not negative infinity, we know T1 is a node (not a leaf) and
let (L1, k1, p1, R1) be its contents (we need not match on TLeaf . The key k1 needs to be at
the root since it has the highest priority, and L1 needs to be its left child since all other keys

are greater than k1. This leaves us with R1, k, and T2. These can just be joined recursively
with join(R1, k, p, T2). We know that R1 < k < T2, so the arguments are valid. There is a
symmetric case when priority(T1) < priority(T2).
Cost of Join. Calculating the cost of join is straightforward. In particular on each step it
either finishes, goes down one level in T1, or goes down one level in T2. In each case the
work before the recursive call is constant. Therefore the overall work for join is bounded
by h(T1) + h(T2). As stated the height of a treap T is bounded by O(log |T |) with high
probability. This means the cost of join is O(log |T1| + log |T2|) = O(log(|T1| + |T2|) with
high probability.
Cost of Split. We now analyze the cost of the split algorithm based on our implementa-
tion of joinMid .
We note that the split algorithm traverses the tree visiting each level once. Now at each
level it does constant work plus the work of the joinM . Notice, however, that the key k′
used in the joinM (Lr , k,′ R) (or joinM (L, k′, Rl)) has a higher priority than either of the
two trees Lr and R. This is because in the input tree, it was above both subtrees, and by
the treap invariant must have had a higher priority. Therefore the join as described above
will take constant work—it just needs to check that the priority of the key is greater than
the priority of both trees, and can then on success will make the treap node immediately.
Therefore the overall cost of each recursive call in split is constant, and the overall cost of
split(T ) is O(h(|T |)), which is O(log |T |) with high probability.
